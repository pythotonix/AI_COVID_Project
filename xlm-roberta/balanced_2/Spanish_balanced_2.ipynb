{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f91ec4d-2dfd-410c-aec6-5c5e5fddeb63",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-12-03T06:59:48.118249Z",
     "iopub.status.busy": "2025-12-03T06:59:48.117478Z",
     "iopub.status.idle": "2025-12-03T07:01:23.896697Z",
     "shell.execute_reply": "2025-12-03T07:01:23.895787Z",
     "shell.execute_reply.started": "2025-12-03T06:59:48.118219Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Note: you may need to restart the kernel to use updated packages.\n",
      "\u001b[2K     \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m43.7/43.7 kB\u001b[0m \u001b[31m1.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[2K     \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m61.0/61.0 kB\u001b[0m \u001b[31m4.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m9.5/9.5 MB\u001b[0m \u001b[31m61.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m:00:01\u001b[0m\n",
      "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m547.8/547.8 kB\u001b[0m \u001b[31m33.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m324.4/324.4 kB\u001b[0m \u001b[31m22.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m18.3/18.3 MB\u001b[0m \u001b[31m85.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m38.3/38.3 MB\u001b[0m \u001b[31m50.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m294.9/294.9 kB\u001b[0m \u001b[31m21.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m3.6/3.6 MB\u001b[0m \u001b[31m79.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m\n",
      "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m566.1/566.1 kB\u001b[0m \u001b[31m39.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m116.3/116.3 kB\u001b[0m \u001b[31m7.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m316.1/316.1 kB\u001b[0m \u001b[31m23.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m363.4/363.4 MB\u001b[0m \u001b[31m5.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m13.8/13.8 MB\u001b[0m \u001b[31m101.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m24.6/24.6 MB\u001b[0m \u001b[31m84.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m883.7/883.7 kB\u001b[0m \u001b[31m52.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m664.8/664.8 MB\u001b[0m \u001b[31m2.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m211.5/211.5 MB\u001b[0m \u001b[31m7.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m56.3/56.3 MB\u001b[0m \u001b[31m2.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m0:00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m127.9/127.9 MB\u001b[0m \u001b[31m6.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m207.5/207.5 MB\u001b[0m \u001b[31m8.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m21.1/21.1 MB\u001b[0m \u001b[31m58.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m143.5/143.5 kB\u001b[0m \u001b[31m11.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25h\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
      "bigframes 2.12.0 requires google-cloud-bigquery-storage<3.0.0,>=2.30.0, which is not installed.\n",
      "opentelemetry-proto 1.37.0 requires protobuf<7.0,>=5.0, but you have protobuf 4.25.8 which is incompatible.\n",
      "s3fs 2025.3.0 requires fsspec==2025.3.0.*, but you have fsspec 2024.5.0 which is incompatible.\n",
      "a2a-sdk 0.3.10 requires protobuf>=5.29.5, but you have protobuf 4.25.8 which is incompatible.\n",
      "ray 2.51.1 requires click!=8.3.0,>=7.0, but you have click 8.3.0 which is incompatible.\n",
      "cesium 0.12.4 requires numpy<3.0,>=2.0, but you have numpy 1.26.4 which is incompatible.\n",
      "google-colab 1.0.0 requires notebook==6.5.7, but you have notebook 6.5.4 which is incompatible.\n",
      "google-colab 1.0.0 requires pandas==2.2.2, but you have pandas 2.2.3 which is incompatible.\n",
      "google-colab 1.0.0 requires requests==2.32.3, but you have requests 2.32.5 which is incompatible.\n",
      "google-colab 1.0.0 requires tornado==6.4.2, but you have tornado 6.5.2 which is incompatible.\n",
      "dopamine-rl 4.1.2 requires gymnasium>=1.0.0, but you have gymnasium 0.29.0 which is incompatible.\n",
      "bigframes 2.12.0 requires rich<14,>=12.4.4, but you have rich 14.2.0 which is incompatible.\n",
      "thinc 8.3.6 requires numpy<3.0.0,>=2.0.0, but you have numpy 1.26.4 which is incompatible.\n",
      "opencv-contrib-python 4.12.0.88 requires numpy<2.3.0,>=2; python_version >= \"3.9\", but you have numpy 1.26.4 which is incompatible.\n",
      "libcugraph-cu12 25.6.0 requires libraft-cu12==25.6.*, but you have libraft-cu12 25.2.0 which is incompatible.\n",
      "opencv-python 4.12.0.88 requires numpy<2.3.0,>=2; python_version >= \"3.9\", but you have numpy 1.26.4 which is incompatible.\n",
      "gradio 5.38.1 requires pydantic<2.12,>=2.0, but you have pydantic 2.12.4 which is incompatible.\n",
      "cudf-polars-cu12 25.6.0 requires pylibcudf-cu12==25.6.*, but you have pylibcudf-cu12 25.2.2 which is incompatible.\n",
      "pydrive2 1.21.3 requires cryptography<44, but you have cryptography 46.0.3 which is incompatible.\n",
      "pydrive2 1.21.3 requires pyOpenSSL<=24.2.1,>=19.1.0, but you have pyopenssl 25.3.0 which is incompatible.\n",
      "imbalanced-learn 0.13.0 requires scikit-learn<2,>=1.3.2, but you have scikit-learn 1.2.2 which is incompatible.\n",
      "ydf 0.13.0 requires protobuf<7.0.0,>=5.29.1, but you have protobuf 4.25.8 which is incompatible.\n",
      "opencv-python-headless 4.12.0.88 requires numpy<2.3.0,>=2; python_version >= \"3.9\", but you have numpy 1.26.4 which is incompatible.\n",
      "plotnine 0.14.5 requires matplotlib>=3.8.0, but you have matplotlib 3.7.2 which is incompatible.\n",
      "pylibcugraph-cu12 25.6.0 requires pylibraft-cu12==25.6.*, but you have pylibraft-cu12 25.2.0 which is incompatible.\n",
      "pylibcugraph-cu12 25.6.0 requires rmm-cu12==25.6.*, but you have rmm-cu12 25.2.0 which is incompatible.\n",
      "umap-learn 0.5.9.post2 requires scikit-learn>=1.6, but you have scikit-learn 1.2.2 which is incompatible.\n",
      "grpcio-status 1.71.2 requires protobuf<6.0dev,>=5.26.1, but you have protobuf 4.25.8 which is incompatible.\n",
      "gcsfs 2025.3.0 requires fsspec==2025.3.0, but you have fsspec 2024.5.0 which is incompatible.\n",
      "mlxtend 0.23.4 requires scikit-learn>=1.3.1, but you have scikit-learn 1.2.2 which is incompatible.\u001b[0m\u001b[31m\n",
      "\u001b[0mNote: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%pip -q uninstall -y transformers tokenizers datasets accelerate huggingface_hub protobuf numpy pyarrow\n",
    "%pip -q install \\\n",
    "  \"protobuf<5.0.0\" \\\n",
    "  \"transformers==4.44.2\" \\\n",
    "  \"datasets==2.20.0\" \\\n",
    "  \"accelerate==0.34.2\" \\\n",
    "  \"tokenizers>=0.19.1\" \\\n",
    "  \"huggingface_hub>=0.24\" \\\n",
    "  \"safetensors>=0.4\" \\\n",
    "  \"sentencepiece>=0.1.99\" \\\n",
    "  \"numpy==1.26.4\" \\\n",
    "  \"pyarrow==15.0.2\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "81329ecc-01d4-4332-983a-dda699cf1f2d",
   "metadata": {
    "execution": {
     "execution_failed": "2025-12-03T07:01:40.719Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "import os\n",
    "os.kill(os.getpid(), 9)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "396e7cde-265f-413a-a51e-b82a89096daf",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-12-03T07:01:53.023000Z",
     "iopub.status.busy": "2025-12-03T07:01:53.022508Z",
     "iopub.status.idle": "2025-12-03T07:01:55.802250Z",
     "shell.execute_reply": "2025-12-03T07:01:55.801618Z",
     "shell.execute_reply.started": "2025-12-03T07:01:53.022977Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "transformers: 4.44.2\n",
      "datasets: 2.20.0\n",
      "accelerate: 0.34.2\n",
      "protobuf: 4.25.8\n",
      "numpy: 1.26.4\n",
      "pyarrow: 15.0.2\n",
      "python: 3.11.13 | torch: 2.6.0+cu124 | cuda: True\n",
      "gpu: Tesla T4\n"
     ]
    }
   ],
   "source": [
    "import transformers, datasets, accelerate, google.protobuf as gp, numpy, pyarrow\n",
    "import torch, sys, platform\n",
    "print(\"transformers:\", transformers.__version__)\n",
    "print(\"datasets:\", datasets.__version__)\n",
    "print(\"accelerate:\", accelerate.__version__)\n",
    "print(\"protobuf:\", gp.__version__)\n",
    "print(\"numpy:\", numpy.__version__)\n",
    "print(\"pyarrow:\", pyarrow.__version__)\n",
    "print(\"python:\", sys.version.split()[0], \"| torch:\", torch.__version__, \"| cuda:\", torch.cuda.is_available())\n",
    "if torch.cuda.is_available():\n",
    "    print(\"gpu:\", torch.cuda.get_device_name(0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c3de6a6-b3d7-435b-b95a-82ee84ae7a5c",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-12-03T07:02:12.703325Z",
     "iopub.status.busy": "2025-12-03T07:02:12.702651Z",
     "iopub.status.idle": "2025-12-03T07:02:17.576818Z",
     "shell.execute_reply": "2025-12-03T07:02:17.576237Z",
     "shell.execute_reply.started": "2025-12-03T07:02:12.703300Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-12-03 07:02:13.655637: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:477] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
      "E0000 00:00:1764745333.675938     139 cuda_dnn.cc:8310] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "E0000 00:00:1764745333.682134     139 cuda_blas.cc:1418] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n"
     ]
    }
   ],
   "source": [
    "import os, json, numpy as np, torch\n",
    "from datasets import load_from_disk\n",
    "from transformers import (AutoTokenizer, AutoModelForSequenceClassification,\n",
    "                          TrainingArguments, Trainer, EarlyStoppingCallback)\n",
    "\n",
    "BASE_DIR = \"/kaggle/working\"\n",
    "LANG_TAG = \"es\"\n",
    "SAVE_LANG_DIR = f\"{BASE_DIR}/{LANG_TAG}\"\n",
    "TOK_DIR = f\"{SAVE_LANG_DIR}/tok_xlmr_256\"\n",
    "\n",
    "SEED = 42\n",
    "torch.manual_seed(SEED); \n",
    "if torch.cuda.is_available(): torch.cuda.manual_seed_all(SEED)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7179f02b",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-12-03T07:03:07.902241Z",
     "iopub.status.busy": "2025-12-03T07:03:07.901584Z",
     "iopub.status.idle": "2025-12-03T07:04:29.070148Z",
     "shell.execute_reply": "2025-12-03T07:04:29.069438Z",
     "shell.execute_reply.started": "2025-12-03T07:03:07.902208Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7f1ebbc5568a4918b6a45e485aa95db0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json:   0%|          | 0.00/25.0 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9ca3ea3aa1624b6b8e845f285149eada",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/615 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5d62b664ca6d404098d7692d9a4b979d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "sentencepiece.bpe.model:   0%|          | 0.00/5.07M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "93596e94ff604dadb259bf1a2ae4ab1f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.json:   0%|          | 0.00/9.10M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.11/dist-packages/transformers/tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "749f2f17b7c340818b07aebba70b130b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors:   0%|          | 0.00/1.12G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of XLMRobertaForSequenceClassification were not initialized from the model checkpoint at xlm-roberta-base and are newly initialized: ['classifier.dense.bias', 'classifier.dense.weight', 'classifier.out_proj.bias', 'classifier.out_proj.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "tok = load_from_disk(TOK_DIR)\n",
    "with open(f\"{SAVE_LANG_DIR}/label2id.json\",\"r\",encoding=\"utf-8\") as f: label2id = json.load(f)\n",
    "with open(f\"{SAVE_LANG_DIR}/id2label.json\",\"r\",encoding=\"utf-8\") as f: id2label = json.load(f)\n",
    "num_labels = len(label2id)\n",
    "\n",
    "MODEL_NAME = \"xlm-roberta-base\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME)\n",
    "model = AutoModelForSequenceClassification.from_pretrained(\n",
    "    MODEL_NAME, num_labels=num_labels, id2label=id2label, label2id=label2id\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d03f3dc0",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-12-03T07:36:23.517379Z",
     "iopub.status.busy": "2025-12-03T07:36:23.516817Z",
     "iopub.status.idle": "2025-12-03T07:36:23.525145Z",
     "shell.execute_reply": "2025-12-03T07:36:23.524594Z",
     "shell.execute_reply.started": "2025-12-03T07:36:23.517355Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "train_bal_counts = np.bincount(tok[\"train\"][\"labels\"], minlength=num_labels)\n",
    "\n",
    "def class_balanced_weights(counts, beta=0.99):\n",
    "    counts = np.asarray(counts, dtype=float)\n",
    "    eff_num = (1.0 - np.power(beta, counts)) / (1.0 - beta)\n",
    "    w = 1.0 / np.maximum(eff_num, 1e-9)\n",
    "    return (w / w.sum()) * len(w)\n",
    "\n",
    "USE_LIGHT_WEIGHTS = False\n",
    "if USE_LIGHT_WEIGHTS:\n",
    "    w = class_balanced_weights(train_bal_counts, beta=0.99)\n",
    "    class_weights = torch.tensor(w, dtype=torch.float)\n",
    "    print(\"Light class weights:\", w)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e0c6ec40",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-12-03T07:04:42.206119Z",
     "iopub.status.busy": "2025-12-03T07:04:42.205522Z",
     "iopub.status.idle": "2025-12-03T07:04:42.211906Z",
     "shell.execute_reply": "2025-12-03T07:04:42.211163Z",
     "shell.execute_reply.started": "2025-12-03T07:04:42.206093Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "from sklearn.metrics import accuracy_score, precision_recall_fscore_support\n",
    "\n",
    "def compute_metrics(pred):\n",
    "    y_true = pred.label_ids\n",
    "    y_pred = np.argmax(pred.predictions, axis=1)\n",
    "    acc = accuracy_score(y_true, y_pred)\n",
    "    prec, rec, f1, _ = precision_recall_fscore_support(y_true, y_pred, average=\"macro\", zero_division=0)\n",
    "    return {\"accuracy\": acc, \"precision_macro\": prec, \"recall_macro\": rec, \"f1_macro\": f1}\n",
    "\n",
    "from transformers import Trainer\n",
    "import torch.nn as nn\n",
    "\n",
    "class WeightedCELossTrainer(Trainer):\n",
    "    def __init__(self, class_weights: torch.Tensor, *args, **kwargs):\n",
    "        super().__init__(*args, **kwargs)\n",
    "        self.class_weights = class_weights\n",
    "    def compute_loss(self, model, inputs, return_outputs=False):\n",
    "        labels = inputs[\"labels\"]\n",
    "        outputs = model(**inputs)\n",
    "        logits = outputs.logits\n",
    "        loss_fct = nn.CrossEntropyLoss(weight=self.class_weights.to(logits.device))\n",
    "        loss = loss_fct(logits, labels)\n",
    "        return (loss, outputs) if return_outputs else loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "115bc1fe",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-12-03T07:04:52.474648Z",
     "iopub.status.busy": "2025-12-03T07:04:52.473916Z",
     "iopub.status.idle": "2025-12-03T07:21:44.515687Z",
     "shell.execute_reply": "2025-12-03T07:21:44.514632Z",
     "shell.execute_reply.started": "2025-12-03T07:04:52.474624Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.11/dist-packages/transformers/training_args.py:1525: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of ğŸ¤— Transformers. Use `eval_strategy` instead\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.11/dist-packages/accelerate/accelerator.py:494: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.\n",
      "  self.scaler = torch.cuda.amp.GradScaler(**kwargs)\n",
      "/usr/local/lib/python3.11/dist-packages/torch/nn/parallel/_functions.py:70: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='1143' max='1143' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [1143/1143 16:46, Epoch 3/3]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Accuracy</th>\n",
       "      <th>Precision Macro</th>\n",
       "      <th>Recall Macro</th>\n",
       "      <th>F1 Macro</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>0.487400</td>\n",
       "      <td>0.344896</td>\n",
       "      <td>0.866314</td>\n",
       "      <td>0.684901</td>\n",
       "      <td>0.804298</td>\n",
       "      <td>0.725917</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>0.328600</td>\n",
       "      <td>0.537754</td>\n",
       "      <td>0.809927</td>\n",
       "      <td>0.645720</td>\n",
       "      <td>0.857094</td>\n",
       "      <td>0.700717</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>0.251400</td>\n",
       "      <td>0.533194</td>\n",
       "      <td>0.839444</td>\n",
       "      <td>0.662893</td>\n",
       "      <td>0.876387</td>\n",
       "      <td>0.727788</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.11/dist-packages/torch/nn/parallel/_functions.py:70: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.11/dist-packages/torch/nn/parallel/_functions.py:70: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.11/dist-packages/torch/nn/parallel/_functions.py:70: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved best model to: /kaggle/working/es/xlmr_es_best\n"
     ]
    }
   ],
   "source": [
    "USE_BF16 = torch.cuda.is_available() and torch.cuda.get_device_capability(0)[0] >= 8\n",
    "USE_FP16 = torch.cuda.is_available() and not USE_BF16\n",
    "\n",
    "OUTPUT_DIR = f\"{SAVE_LANG_DIR}/xlmr_{LANG_TAG}_sentiment\"\n",
    "\n",
    "args = TrainingArguments(\n",
    "    output_dir=OUTPUT_DIR,\n",
    "    evaluation_strategy=\"epoch\",\n",
    "    save_strategy=\"epoch\",\n",
    "    logging_strategy=\"steps\",\n",
    "    logging_steps=50,\n",
    "    load_best_model_at_end=True,\n",
    "    metric_for_best_model=\"f1_macro\",\n",
    "    greater_is_better=True,\n",
    "    num_train_epochs=3,\n",
    "    per_device_train_batch_size=8,\n",
    "    per_device_eval_batch_size=16,\n",
    "    learning_rate=2e-5,\n",
    "    warmup_ratio=0.1,\n",
    "    weight_decay=0.01,\n",
    "    seed=SEED,\n",
    "    report_to=\"none\",\n",
    "    fp16=USE_FP16,\n",
    "    bf16=USE_BF16,\n",
    ")\n",
    "\n",
    "if USE_LIGHT_WEIGHTS:\n",
    "    trainer = WeightedCELossTrainer(\n",
    "        class_weights=class_weights,\n",
    "        model=model,\n",
    "        args=args,\n",
    "        train_dataset=tok[\"train\"],\n",
    "        eval_dataset=tok[\"test\"],\n",
    "        tokenizer=tokenizer,\n",
    "        compute_metrics=compute_metrics,\n",
    "        callbacks=[EarlyStoppingCallback(early_stopping_patience=2)],\n",
    "    )\n",
    "else:\n",
    "    trainer = Trainer(\n",
    "        model=model,\n",
    "        args=args,\n",
    "        train_dataset=tok[\"train\"],\n",
    "        eval_dataset=tok[\"test\"],\n",
    "        tokenizer=tokenizer,\n",
    "        compute_metrics=compute_metrics,\n",
    "        callbacks=[EarlyStoppingCallback(early_stopping_patience=2)],\n",
    "    )\n",
    "\n",
    "trainer.train()\n",
    "\n",
    "BEST_DIR = f\"{SAVE_LANG_DIR}/xlmr_{LANG_TAG}_best\"\n",
    "model_to_save = trainer.model\n",
    "model_to_save.save_pretrained(BEST_DIR)\n",
    "tokenizer.save_pretrained(BEST_DIR)\n",
    "with open(f\"{BEST_DIR}/label2id.json\",\"w\",encoding=\"utf-8\") as f: json.dump(label2id,f,ensure_ascii=False,indent=2)\n",
    "with open(f\"{BEST_DIR}/id2label.json\",\"w\",encoding=\"utf-8\") as f: json.dump(id2label,f,ensure_ascii=False,indent=2)\n",
    "print(\"Saved best model to:\", BEST_DIR)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "43b30dd2-e703-44d2-bb25-f22bbf95d88b",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-12-03T07:25:41.622447Z",
     "iopub.status.busy": "2025-12-03T07:25:41.621862Z",
     "iopub.status.idle": "2025-12-03T07:27:57.286677Z",
     "shell.execute_reply": "2025-12-03T07:27:57.286000Z",
     "shell.execute_reply.started": "2025-12-03T07:25:41.622422Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model zipped successfully:\n",
      "/kaggle/working/xlmr_es_best.zip\n"
     ]
    }
   ],
   "source": [
    "import shutil, os\n",
    "MODEL_DIR = f\"/kaggle/working/{LANG_TAG}/xlmr_{LANG_TAG}_best\"\n",
    "ZIP_PATH  = f\"/kaggle/working/xlmr_{LANG_TAG}_best.zip\"\n",
    "\n",
    "shutil.make_archive(ZIP_PATH.replace(\".zip\", \"\"), 'zip', MODEL_DIR)\n",
    "print(f\"Model zipped successfully:\\n{ZIP_PATH}\")"
   ]
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "nvidiaTeslaT4",
   "dataSources": [],
   "dockerImageVersionId": 31193,
   "isGpuEnabled": true,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
